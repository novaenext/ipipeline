# ipipeline

A micro framework for building and executing pipelines from different domains.

## Features

- **Simplicity:** high-level interfaces that can be used to perform complex tasks.

- **Flexibility:** freedom to build the pipeline according to the requirements of the problem.

- **Scalability:** pipeline execution through concurrency or parallelism (coming soon).

## Installation

ipipeline is installed from the Python Package Index (PyPI).

```shell
pip install ipipeline
```

## Documentation

To learn how this package works, follow the [documentation]() (coming soon).

## Contribution

To learn how to contribute to this repository, follow the [contribution](https://github.com/novaenext/ipipeline/blob/master/CONTRIBUTING.md) file.

## License

To learn about the legal rights linked to this repository, follow the [license](https://github.com/novaenext/ipipeline/blob/master/LICENSE.md) file.

## Example

This example was divided into sections to explain the main features of the package. In case of questions about a specific detail the package contains docstrings for all modules, classes, methods and functions.

**Imports:**

The ipipeline package tries to keep things simple, therefore all the work is done through the pipeline interface imported as Pipeline and the execution interface imported as SequentialExecutor.

```python
import logging

from ipipeline.control import SequentialExecutor
from ipipeline.structure import Pipeline


logging.basicConfig(
    format='[%(asctime)s] %(levelname)s %(name)s - %(message)s', 
    datefmt='%Y-%m-%d %H:%M:%S', 
    level=logging.INFO
)
```

**Tasks:**

The functions below represent the user tasks that need to be executed in a certain order which forms a workflow with the following idea: data is collected from somewhere, then processed in two different ways and finally displayed on the screen. Although this example only contains functions, the methods of an instance can also be used.

```python
def collect() -> list:
    return [1, 2]


def process1(x: int) -> int:
    return x + 1


def process2(y: int) -> int:
    return y * 2


def display(x: int, y: int, z: int) -> None:
    print(f'results - x: {x}, y: {y}, z: {z}')
```

**Pipeline:**

A pipeline is the entry point for the user code, through it the nodes (tasks) and connections (relationships between tasks) added are represented as a graph (workflow). The graph is used by the executor and is not visible to the user.

```python
pipeline = Pipeline('p1', tags=['example'])
pipeline.add_node(
    'n1', collect, outputs=['x', 'y'], tags=['collect']
)
pipeline.add_node(
    'n2', process1, inputs={'x': 'c.x'}, outputs=['x'], tags=['process1']
)
pipeline.add_node(
    'n3', process2, inputs={'y': 'c.y'}, outputs=['y'], tags=['process2']
)
pipeline.add_node(
    'n4', display, inputs={'x': 'c.x', 'y': 'c.y', 'z': 8}, tags=['display']
)
pipeline.add_conn('c1', 'n1', 'n2')
pipeline.add_conn('c2', 'n1', 'n3')
pipeline.add_conn('c3', 'n2', 'n4')
pipeline.add_conn('c4', 'n3', 'n4')
```

Based on the workflow defined in the declarations section, the pipeline was built with four nodes and four connections. Two aspects deserve attention here, the inputs and outputs parameters of the add_node method.

The outputs parameter, when declared, indicates that during the pipeline execution, the function returns must be stored in the catalog with specific names. For example, the outputs parameter of the 'n1' node expects to store two items in the catalog with the names 'x' and 'y' which are obtained from the returns of the collect function.

The inputs parameter, when declared, indicates that during the pipeline execution, the function receives a dictionary with its arguments. For example, the inputs parameter of the 'n4' node expects to receive a dictionary where the 'x' and 'y' values are obtained from the catalog ('c.<item_id>') and the 'z' value is obtained directly. The 'c.' prefix assumes the existence of an item in the catalog stored by a predecessor node.

The connections determine the order in which the nodes are executed. For example, 'c1' connection indicates a relationship between 'n1' node (source) and 'n2' node (destination) where the 'n2' node depends on the execution of the 'n1' node. A node can dependent on another node even though it does not use the outputs of its predecessor.

**Executor:**

An executor is responsible for executing a pipeline from the topological order (execution order) of the graph. Therefore, it is expected that the connections between the nodes form a DAG (Directed Acyclic Graph), if this does not happen, an error is raised. Behind the scenes, a catalog is created to store the node returns that are requested by other nodes during the execution.

```python
executor = SequentialExecutor(pipeline)
executor.execute_pipeline(executor.obtain_topo_order())
```

Below are the log results generated by the execution. It is recommended to turn off the logs in cases where there are many nodes or the pipeline is called many times inside a loop.

```shell
[2021-10-26 12:00:21] INFO ipipeline.control.execution - topo_order: [['n1'], ['n2', 'n3'], ['n4']]
[2021-10-26 12:00:21] INFO ipipeline.control.execution - node - id: n1, tags: ['collect']
[2021-10-26 12:00:21] INFO ipipeline.control.execution - node - id: n2, tags: ['process1']
[2021-10-26 12:00:21] INFO ipipeline.control.execution - node - id: n3, tags: ['process2']
[2021-10-26 12:00:21] INFO ipipeline.control.execution - node - id: n4, tags: ['display']
results - x: 2, y: 4, z: 8
```

According to the defined workflow, the nodes were executed in the expected order. The inner lists of the topological order must always be executed in order, however, the elements within them can be executed simultaneously. As in this example the SequentialExecutor class was used, the nodes were executed as if the topological order were a flat list.
